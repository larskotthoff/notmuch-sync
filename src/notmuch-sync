#!/usr/bin/env python

"""notmuch-sync: Synchronize notmuch email databases and message files between local and remote systems."""

import argparse
import asyncio
import hashlib
import json
import logging
import os
import shlex
import shutil
import struct
import subprocess
import sys

from pathlib import Path
from select import select

import notmuch2

logging.basicConfig(format="[{asctime}] {message}", style="{")
logger = logging.getLogger(__name__)

transfer = {"read": 0, "write": 0}

def write(data, stream):
    """
    Write data to a stream with a 4-byte length prefix.

    Args:
        data (bytes): The data to write.
        stream: A writable stream supporting .write() and .flush().
    """
    stream.write(struct.pack("!I", len(data)))
    transfer["write"] += 4
    stream.write(data)
    transfer["write"] += len(data)
    stream.flush()


def read(stream):
    """
    Read 4-byte length-prefixed data from a stream.

    Args:
        stream: A readable stream supporting .read().

    Returns:
        bytes: The data read from the stream.
    """
    size_data = stream.read(4)
    transfer["read"] += 4
    size = struct.unpack("!I", size_data)[0]
    transfer["read"] += size
    return stream.read(size)


def get_changes(db, revision, prefix, sync_file):
    """
    Get changes that happened since the last sync, or everything in the DB if no previous sync.

    Args:
        db: An open notmuch2.Database object.
        revision: Database revision object, must have .uuid and .rev.
        prefix (str): Prefix path for filenames (notmuch config database.path).
        sync_file (str): Path to the file storing the sync state.

    Returns:
        dict: Mapping of message IDs to their tags and files.
    """
    rev_prev = -1
    try:
        with open(sync_file, 'r', encoding="utf-8") as f:
            tmp = f.read().strip('\n\r').split(' ')
            uuid = revision.uuid.decode()
            try:
                if tmp[1] != uuid:
                    sys.exit(f"Last sync with UUID {tmp[1]}, but notmuch DB has UUID {uuid}, aborting...")
                rev_prev = int(tmp[0])
                if rev_prev > revision.rev:
                    sys.exit(f"Last sync revision {rev_prev} larger than current DB revision {revision.rev}, aborting...")
            except Exception:
                sys.exit(f"Sync state file '{sync_file}' corrupted, delete to sync from scratch.")
    except FileNotFoundError:
        # no previous sync or sync file broken, leave rev_prev at 0 as this will sync entire DB
        pass

    return {msg.messageid: {"tags": list(msg.tags), "files": [
        {"name": str(f).removeprefix(prefix),
         "sha": hashlib.new("sha256", Path(f).read_bytes()).hexdigest()} for f in msg.filenames()
        ]} for msg in db.messages(f"lastmod:{rev_prev + 1}..")}


def sync_tags(db, changes_mine, changes_theirs):
    """
    Synchronize tags between local and remote changes. Applies tags from all
    remotely changed IDs to local messages with the same ID, overwriting any
    local tags. If an ID appears both in remote and local changes, take the
    union of all tags. If a message is not found locally, do nothing (will be
    synced later).

    Args:
        db: An open notmuch2.Database object.
        changes_mine (dict): Local changes, mapping message IDs to tags.
        changes_theirs (dict): Remote changes, mapping message IDs to tags.

    Returns:
        int: Number of tag changes made.
    """
    changes = 0
    for mid in changes_theirs:
        tags = changes_theirs[mid]["tags"]
        if mid in changes_mine:
            tags = set(tags) | set(changes_mine[mid]["tags"])
        tags = set(tags)
        try:
            msg = db.find(mid)
            if msg.ghost:
                continue
            if tags != set(msg.tags):
                logger.info("Setting tags %s for %s.", sorted(list(tags)), mid)
                with msg.frozen():
                    changes += 1
                    msg.tags.clear()
                    for tag in sorted(list(tags)):
                        msg.tags.add(tag)
                    msg.tags.to_maildir_flags()
        except LookupError:
            # we don't have this message on our side, it will be added later
            # when syncing files
            pass

    return changes


def initial_sync(from_stream, to_stream):
    """
    Perform the initial synchronization of UUIDs and tag changes, which includes
    applying any remote tag changes to messages that exist locally. UUIDs and
    changes are communicated to/from the remote over the respective streams.
    Writes sync state file at the end.

    Args:
        from_stream: Stream to read from the remote.
        to_stream: Stream to write to the remote.

    Returns:
        tuple: (prefix, local changes dict, remote changes dict, number of local
                tag changes)
    """
    # only do tag sync initially to avoid locking db during lengthy file transfers
    with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
        prefix = os.path.join(str(dbw.default_path()), '')
        revision = dbw.revision()
        # using a hash here b/c of Python's scoping rules -- need to set this in
        # the async function
        uuids = {}
        uuids["mine"] = revision.uuid.decode()

        async def send_uuid():
            logger.info("Sending UUID...")
            to_stream.write(uuids["mine"].encode("utf-8"))
            transfer["write"] += 36
            to_stream.flush()

        async def recv_uuid():
            logger.info("Receiving UUID...")
            uuids["theirs"] = from_stream.read(36).decode("utf-8")
            transfer["read"] += 36

        async def sync_uuids():
            await asyncio.gather(send_uuid(), recv_uuid())

        asyncio.run(sync_uuids())
        logger.info("UUIDs synced.")
        fname = os.path.join(prefix, ".notmuch", f"notmuch-sync-{uuids["theirs"]}")

        changes = {}
        logger.info("Computing local changes...")
        changes["mine"] = get_changes(dbw, revision, prefix, fname)

        async def send():
            logger.info("Sending local changes...")
            write(json.dumps(changes["mine"]).encode("utf-8"), to_stream)

        async def recv():
            logger.info("Receiving remote changes...")
            changes["theirs"] = json.loads(read(from_stream).decode("utf-8"))

        async def sync():
            await asyncio.gather(send(), recv())

        asyncio.run(sync())
        logger.info("Changes synced.")
        tchanges = sync_tags(dbw, changes["mine"], changes["theirs"])
        logger.info("Tags synced.")

        # record last sync here -- this means that later changes (adding files,
        # adding messages, deleting) will show up during the next sync, but we
        # check whether there are any actual changes, so they shouldn't result
        # in a cascading torrent
        # if we don't record the last sync here, we might miss external changes
        # during the next stages, as the DB is no longer locked
        with open(fname, 'w', encoding="utf-8") as f:
            revision = dbw.revision()
            logger.info("Writing last sync revision %s.", revision.rev)
            f.write(f"{revision.rev} {revision.uuid.decode()}")

    return (prefix, changes["mine"], changes["theirs"], tchanges)


def get_missing_files(changes_mine, changes_theirs, prefix):
    """
    Determine which files are missing locally compared to the remote, and handle
    file moves/copies based on SHA256 checksums. Delete any files that aren't
    there on the remote anymore, unless the message ID was also changed locally.
    This never deletes a message, only duplicate files for a message.

    Args:
        changes_mine (dict): Local changes.
        changes_theirs (dict): Remote changes.
        prefix (str): Prefix path for filenames (notmuch config database.path).

    Returns:
        tuple: (dict of missing files, number of local moves/copies, number of
                local deletions)
    """
    ret = {}
    mcchanges = 0
    dchanges = 0
    with notmuch2.Database() as db:
        for mid in changes_theirs:
            try:
                msg = db.find(mid)
                if msg.ghost:
                    ret[mid] = changes_theirs[mid]
                    continue
                fnames_theirs = [ f["name"] for f in changes_theirs[mid]["files"] ]
                fnames_mine = [ str(f).removeprefix(prefix) for f in msg.filenames() ]
                missing_mine = [ f for f in fnames_theirs if f not in fnames_mine ]
                if len(missing_mine) > 0:
                    hashes_mine = [{"name": str(f).removeprefix(prefix),
                                    "sha": hashlib.new("sha256", Path(f).read_bytes()).hexdigest()}
                                    for f in msg.filenames()]
                    for f in changes_theirs[mid]["files"]:
                        if f["name"] in missing_mine:
                            # check if it has been moved/copied
                            matches = [ x for x in hashes_mine if f["sha"] == x["sha"] ]
                            if len(matches) > 0:
                                mcchanges += 1
                                src = os.path.join(prefix, matches[0]["name"])
                                dst = os.path.join(prefix, f["name"])
                                # if there's a local change for this ID, copy
                                # this is to prevent inconsistencies when
                                # changes happen on both sides
                                if matches[0] in changes_theirs[mid]["files"] or mid in changes_mine:
                                    logger.info("Copying %s to %s.", src, dst)
                                    shutil.copy(src, dst)
                                    fnames_mine.append(f["name"])
                                else:
                                    logger.info("Moving %s to %s.", src, dst)
                                    shutil.move(src, dst)
                                    fnames_mine.append(f["name"])
                                    fnames_mine.remove(matches[0]["name"])
                                with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
                                    dbw.add(dst)
                                    # see above comment for local changes check
                                    if matches[0] not in changes_theirs[mid]["files"] and mid not in changes_mine:
                                        logger.info("Removing %s from DB.", src)
                                        dbw.remove(src)
                                missing_mine.remove(f["name"])
                # check which ones are still missing
                if len(missing_mine) > 0:
                    ret[mid] = {"files": [f for f in changes_theirs[mid]["files"] if f["name"] in missing_mine]}

                # delete any files that are not there remotely after copy/move
                if mid not in changes_mine:
                    if len(set(fnames_mine).intersection(fnames_theirs)) == 0:
                        raise ValueError(f"Message '{mid}' has {fnames_theirs} on remote and different {fnames_mine} locally!")
                    to_delete = set(fnames_mine) - set(fnames_theirs)
                    with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
                        for f in to_delete:
                            fname = os.path.join(prefix, f)
                            dchanges += 1
                            logger.info("Removing %s from DB and deleting file.", fname)
                            dbw.remove(fname)
                            Path(fname).unlink()
            except LookupError:
                # don't have this message; all files missing
                ret[mid] = changes_theirs[mid]
    return (ret, mcchanges, dchanges)


def send_file(fname, stream):
    """
    Send a file's contents to a stream with 4-byte length prefix.

    Args:
        fname (str): Path to the file to send.
        stream: Writable stream.
    """
    with open(fname, "rb") as f:
        write(f.read(), stream)


def recv_file(fname, stream, sha):
    """
    Receive a file with a 4-byte length prefix from a stream and write it to
    disk, validating its checksum.

    Args:
        fname (str): Destination file path.
        stream: Readable stream.
        sha (str): Expected SHA256 checksum.

    Raises:
        ValueError: If received file's checksum does not match expected.
    """
    content = read(stream)
    sha_mine = hashlib.new("sha256", content).hexdigest()
    if sha_mine != sha:
        raise ValueError(f"Checksum of received file '{fname}' ({sha_mine}) does not match expected ({sha})!")
    Path(fname).parent.mkdir(parents=True, exist_ok=True)
    with open(fname, "wb") as f:
        f.write(content)


def sync_files(prefix, missing, from_stream, to_stream):
    """
    Synchronize files that are missing locally or remotely.

    Args:
        prefix (str): Prefix path for filenames (notmuch config database.path).
        missing (dict): Mapping of missing files by message ID.
        from_stream: Stream to read file names and files from.
        to_stream: Stream to send file names and files to.

    Returns:
        tuple: (number of locally added messages, number of locally added files)
    """
    files = {}
    files["mine"] = [ f | {"id": mid} for mid in missing for f in missing[mid]["files"] ]
    changes = {"files": len(files["mine"]), "messages": 0}

    async def send_fnames():
        logger.info("Sending file names missing on local...")
        to_stream.write(struct.pack("!I", len(files["mine"])))
        transfer["write"] += 4
        to_stream.flush()
        for f in files["mine"]:
            write(f["name"].encode("utf-8"), to_stream)

    async def recv_fnames():
        logger.info("Receving file names missing on remote...")
        size_data = from_stream.read(4)
        transfer["read"] += 4
        nfiles = struct.unpack("!I", size_data)[0]
        files["theirs"] = [ read(from_stream).decode("utf-8") for _ in range(nfiles) ]

    async def sync_fnames():
        await asyncio.gather(send_fnames(), recv_fnames())

    asyncio.run(sync_fnames())
    logger.info("Missing file names synced.")

    async def send_files():
        for idx, fname in enumerate(files["theirs"]):
            logger.info("%s/%s Sending %s...", idx + 1, len(files["theirs"]),
                        fname)
            send_file(os.path.join(prefix, fname), to_stream)

    async def recv_files():
        for idx, f in enumerate(files["mine"]):
            logger.info("%s/%s Receiving %s...", idx + 1, len(files["mine"]), f["name"])
            dst = os.path.join(prefix, f["name"])
            recv_file(dst, from_stream, f["sha"])
            with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
                logger.info("Adding %s to DB.", dst)
                msg, dup = dbw.add(dst)
                if not dup:
                    changes["messages"] += 1
                    with msg.frozen():
                        logger.info("Setting tags %s for received %s.",
                                    sorted(missing[f["id"]]["tags"]),
                                    msg.messageid)
                        msg.tags.clear()
                        for tag in missing[f["id"]]["tags"]:
                            msg.tags.add(tag)

    async def sync():
        await asyncio.gather(send_files(), recv_files())

    asyncio.run(sync())
    logger.info("Missing files synced.")

    return (changes["messages"], changes["files"])


# Separate methods for local and remote to avoid sending all IDs both ways --
# have local figure out what needs to be deleted on both sides
def sync_deletes_local(from_stream, to_stream):
    """
    Synchronize deletions for the local database and instruct remote to delete
    messages/files as needed.

    Args:
        from_stream: Stream to read from the remote.
        to_stream: Stream to write to the remote.

    Returns:
        int: Number of deletions performed locally.
    """
    ids = {}
    dels = {'a': 0}

    async def get_ids():
        with notmuch2.Database() as db:
            logger.info("Getting all message IDs from DB...")
            msgs = db.messages('*')
            ids["mine"] = [ msg.messageid for msg in msgs ]

    async def recv_ids():
        logger.info("Receiving all message IDs from remote...")
        size_data = from_stream.read(4)
        transfer["read"] += 4
        nids = struct.unpack("!I", size_data)[0]
        ids["theirs"] = [ read(from_stream).decode("utf-8") for _ in range(nids) ]

    async def sync_ids():
        await asyncio.gather(get_ids(), recv_ids())

    asyncio.run(sync_ids())
    logger.info("Message IDs synced.")

    async def send_del_ids():
        to_del_remote = set(ids["theirs"]) - set(ids["mine"])
        logger.debug("Remote IDs to be deleted %s.", to_del_remote)
        logger.info("Sending message IDs to be deleted to remote...")
        to_stream.write(struct.pack("!I", len(to_del_remote)))
        transfer["write"] += 4
        to_stream.flush()
        for mid in to_del_remote:
            write(mid.encode("utf-8"), to_stream)

    async def del_ids():
        to_del = set(ids["mine"]) - set(ids["theirs"])
        logger.debug("Local IDs to be deleted %s.", to_del)
        with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
            for mid in to_del:
                dels["a"] += 1
                logger.info("Removing %s from DB and deleting files.", mid)
                try:
                    msg = dbw.find(mid)
                    if msg.ghost:
                        continue
                    for f in msg.filenames():
                        logger.debug("Removing %s.", f)
                        dbw.remove(f)
                        Path(f).unlink()
                except LookupError:
                    # already deleted? doesn't matter
                    pass

    async def sync_dels():
        await asyncio.gather(send_del_ids(), del_ids())

    asyncio.run(sync_dels())
    return dels["a"]


def sync_deletes_remote(from_stream, to_stream):
    """
    Receive instructions from local to delete messages/files from the remote database.

    Args:
        from_stream: Stream to read from the local.
        to_stream: Stream to write to the local.

    Returns:
        int: Number of deletions performed remotely.
    """
    dels = 0
    with notmuch2.Database() as db:
        msgs = db.messages('*')
        ids = [ msg.messageid for msg in msgs ]

    to_stream.write(struct.pack("!I", len(ids)))
    transfer["write"] += 4
    to_stream.flush()
    for i in ids:
        write(i.encode("utf-8"), to_stream)

    size_data = from_stream.read(4)
    transfer["read"] += 4
    nids = struct.unpack("!I", size_data)[0]
    to_del = [ read(from_stream).decode("utf-8") for _ in range(nids) ]

    with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
        for mid in to_del:
            dels += 1
            try:
                msg = dbw.find(mid)
                if msg.ghost:
                    continue
                for f in msg.filenames():
                    dbw.remove(f)
                    Path(f).unlink()
            except LookupError:
                # already deleted? doesn't matter
                pass
    return dels


def sync_remote(args):
    """
    Run synchronization in remote mode.

    Args:
        args: Parsed command-line arguments.
    """
    prefix, changes_mine, changes_theirs, tchanges = initial_sync(sys.stdin.buffer, sys.stdout.buffer)
    missing, fchanges, dfchanges = get_missing_files(changes_mine, changes_theirs, prefix)
    rmessages, rfiles = sync_files(prefix, missing, sys.stdin.buffer, sys.stdout.buffer)
    dchanges = 0
    if args.delete:
        dchanges = sync_deletes_remote(sys.stdin.buffer, sys.stdout.buffer)
    sys.stdout.buffer.write(struct.pack("!IIIIII", tchanges, fchanges, dfchanges,
                                        rmessages, dchanges, rfiles))
    sys.stdout.buffer.flush()


def sync_local(args):
    """
    Run synchronization in local mode, communicating with the remote over SSH or
    a custom command.

    Args:
        args: Parsed command-line arguments.
    """
    if args.remote_cmd:
        cmd = shlex.split(args.remote_cmd)
    else:
        rargs = [(f"{args.user}@" if args.user else "") + args.remote, f"{args.path}"]
        if args.delete:
            rargs.append("--delete")
        cmd = shlex.split(args.ssh_cmd) + rargs

    logger.info("Connecting to remote...")
    with subprocess.Popen(
                cmd,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            ) as proc:
        from_remote = proc.stdout
        err_remote = proc.stderr
        to_remote = proc.stdin

        data = b''
        try:
            prefix, changes_mine, changes_theirs, tchanges = initial_sync(from_remote, to_remote)
            logger.debug("Local changes %s, remote changes %s.", changes_mine, changes_theirs)
            missing, fchanges, dfchanges  = get_missing_files(changes_mine, changes_theirs, prefix)
            logger.debug("Local missing files %s.", missing)
            rmessages, rfiles = sync_files(prefix, missing, from_remote, to_remote)
            dchanges = 0
            if args.delete:
                dchanges = sync_deletes_local(from_remote, to_remote)

            logger.info("Getting change numbers from remote...")
            remote_changes = struct.unpack("!IIIIII", from_remote.read(6 * 4))
            transfer["read"] += 6 * 4
        finally:
            ready, _, exc = select([err_remote], [], [], 0)
            if ready and not exc:
                data = err_remote.read()
                # getting zero data on EOF
                if len(data) > 0:
                    print(f"Remote error: {data}", file=sys.stderr)

        to_remote.close()
        from_remote.close()
        err_remote.close()

    logger.warning("local:\t%s new messages,\t%s new files,\t%s files copied/moved,\t%s files deleted,\t%s messages with tag changes,\t%s messages deleted", rmessages, rfiles, fchanges, dfchanges, tchanges, dchanges)
    logger.warning("remote:\t%s new messages,\t%s new files,\t%s files copied/moved,\t%s files deleted,\t%s messages with tag changes,\t%s messages deleted", remote_changes[3], remote_changes[5], remote_changes[1], remote_changes[2], remote_changes[0], remote_changes[4])
    logger.warning("%s/%s bytes received from/sent to remote.", transfer["read"], transfer["write"])

    if len(data) > 0:
        # error output from remote
        sys.exit(1)


def main():
    """
    Entry point for the command-line interface. Parses arguments and dispatches
    to local or remote sync.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("-r", "--remote", type=str, help="remote host to connect to")
    parser.add_argument("-u", "--user", type=str, help="SSH user to use")
    parser.add_argument("-v", "--verbose", action="count", default=0, help="increases verbosity, up to twice (ignored on remote)")
    parser.add_argument("-q", "--quiet", action="store_true", help="do not print any output, overrides --verbose")
    parser.add_argument("-s", "--ssh-cmd", type=str, default="ssh -CTaxq", help="SSH command to use")
    parser.add_argument("-m", "--mbsync", action="store_true", help="sync mbsync files (.mbsyncstate, .uidvalidity)")
    parser.add_argument("-p", "--path", type=str, default=os.path.basename(sys.argv[0]), help="path to notmuch-sync on remote server")
    parser.add_argument("-c", "--remote-cmd", type=str, help="command to run to sync; overrides --remote, --user, --ssh-cmd, --path")
    parser.add_argument("-d", "--delete", action="store_true", help="sync deleted messages (requires listing all messages in notmuch database, potentially expensive)")
    args = parser.parse_args()

    if args.remote or args.remote_cmd:
        if args.verbose == 1:
            logger.setLevel(level=logging.INFO)
        elif args.verbose == 2:
            logger.setLevel(level=logging.DEBUG)
        else:
            logger.setLevel(level=logging.WARNING)

        if args.quiet:
            logger.disabled = True
        sync_local(args)
    else:
        logger.disabled = True
        sync_remote(args)


if __name__ == "__main__":
    main()
