#!/usr/bin/env python3

"""notmuch-sync: Synchronize notmuch email databases and message files between local and remote systems."""

import argparse
import asyncio
import hashlib
import json
import logging
import os
import shlex
import shutil
import struct
import subprocess
import sys

from pathlib import Path
from select import select

import notmuch2

logging.basicConfig(format="[{asctime}] {message}", style="{")
logger = logging.getLogger(__name__)

transfer = {"read": 0, "write": 0}

def digest(data):
    """
    Compute SHA256 digest of data, removing any X-TUID: lines. This is
    nececessary because mbsync adds these lines to keep track of internal
    progress, but they make identical emails that were retrieved separately
    different.

    Args:
        data (bytes): The data to compute the checsum for.

    Returns:
        The computed checksum.
    """
    pat = b"X-TUID: "
    to_digest = data
    start_idx = data.find(pat)
    if start_idx != -1:
        search_start = start_idx + len(pat)
        end_idx = data.find(b"\n", search_start)

        if end_idx != -1:
            to_digest = data[:start_idx] + data[end_idx + 1:]

    return hashlib.new("sha256", to_digest).hexdigest()


def write(data, stream):
    """
    Write data to a stream with a 4-byte length prefix.

    Args:
        data (bytes): The data to write.
        stream: A writable stream supporting .write() and .flush().
    """
    stream.write(struct.pack("!I", len(data)))
    transfer["write"] += 4
    stream.write(data)
    transfer["write"] += len(data)
    stream.flush()


def read(stream):
    """
    Read 4-byte length-prefixed data from a stream.

    Args:
        stream: A readable stream supporting .read().

    Returns:
        bytes: The data read from the stream.
    """
    size_data = stream.read(4)
    transfer["read"] += 4
    size = struct.unpack("!I", size_data)[0]
    transfer["read"] += size
    return stream.read(size)


def get_changes(db, revision, prefix, sync_file):
    """
    Get changes that happened since the last sync, or everything in the DB if no previous sync.

    Args:
        db: An open notmuch2.Database object.
        revision: Database revision object, must have .uuid and .rev.
        prefix (str): Prefix path for filenames (notmuch config database.path).
        sync_file (str): Path to the file storing the sync state.

    Returns:
        dict: Mapping of message IDs to their tags and files.
    """
    rev_prev = -1
    try:
        with open(sync_file, 'r', encoding="utf-8") as f:
            tmp = f.read().strip('\n\r').split(' ')
            uuid = revision.uuid.decode()
            try:
                if tmp[1] != uuid:
                    sys.exit(f"Last sync with UUID {tmp[1]}, but notmuch DB has UUID {uuid}, aborting...")
                rev_prev = int(tmp[0])
                if rev_prev > revision.rev:
                    sys.exit(f"Last sync revision {rev_prev} larger than current DB revision {revision.rev}, aborting...")
            except Exception:
                sys.exit(f"Sync state file '{sync_file}' corrupted, delete to sync from scratch.")
    except FileNotFoundError:
        # no previous sync or sync file broken, leave rev_prev at 0 as this will sync entire DB
        pass

    return {msg.messageid: {"tags": list(msg.tags), "files": [
        {"name": str(f).removeprefix(prefix),
         "sha": digest(Path(f).read_bytes())} for f in msg.filenames()
        ]} for msg in db.messages(f"lastmod:{rev_prev + 1}..")}


def sync_tags(db, changes_mine, changes_theirs):
    """
    Synchronize tags between local and remote changes. Applies tags from all
    remotely changed IDs to local messages with the same ID, overwriting any
    local tags. If an ID appears both in remote and local changes, take the
    union of all tags. If a message is not found locally, do nothing (will be
    synced later).

    Args:
        db: An open notmuch2.Database object.
        changes_mine (dict): Local changes, mapping message IDs to tags.
        changes_theirs (dict): Remote changes, mapping message IDs to tags.

    Returns:
        int: Number of tag changes made.
    """
    changes = 0
    for mid in changes_theirs:
        tags = changes_theirs[mid]["tags"]
        if mid in changes_mine:
            tags = set(tags) | set(changes_mine[mid]["tags"])
        tags = set(tags)
        try:
            msg = db.find(mid)
            if msg.ghost:
                continue
            if tags != set(msg.tags):
                logger.info("Setting tags %s for %s.", sorted(list(tags)), mid)
                with msg.frozen():
                    changes += 1
                    msg.tags.clear()
                    for tag in sorted(list(tags)):
                        msg.tags.add(tag)
                    msg.tags.to_maildir_flags()
        except LookupError:
            # we don't have this message on our side, it will be added later
            # when syncing files
            pass

    return changes


def record_sync(fname, revision):
    """
    Record last sync revision.

    Args:
        fname: File to write to.
        revision: Revision/UUID to record.
    """
    with open(fname, 'w', encoding="utf-8") as f:
        logger.info("Writing last sync revision %s.", revision.rev)
        f.write(f"{revision.rev} {revision.uuid.decode()}")


def initial_sync(from_stream, to_stream):
    """
    Perform the initial synchronization of UUIDs and tag changes, which includes
    applying any remote tag changes to messages that exist locally. UUIDs and
    changes are communicated to/from the remote over the respective streams.

    Args:
        from_stream: Stream to read from the remote.
        to_stream: Stream to write to the remote.

    Returns:
        tuple: (prefix, local changes dict, remote changes dict, number of local
                tag changes, name of sync file, DB revision after initial sync)
    """
    # only do tag sync initially to avoid locking db during lengthy file transfers
    with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
        prefix = os.path.join(str(dbw.default_path()), '')
        revision = dbw.revision()
        # using a hash here b/c of Python's scoping rules -- need to set this in
        # the async function
        uuids = {}
        uuids["mine"] = revision.uuid.decode()

        async def send_uuid():
            logger.info("Sending UUID...")
            to_stream.write(uuids["mine"].encode("utf-8"))
            transfer["write"] += 36
            to_stream.flush()

        async def recv_uuid():
            logger.info("Receiving UUID...")
            uuids["theirs"] = from_stream.read(36).decode("utf-8")
            transfer["read"] += 36

        async def sync_uuids():
            await asyncio.gather(send_uuid(), recv_uuid())

        asyncio.run(sync_uuids())
        logger.info("UUIDs synced.")
        fname = os.path.join(prefix, ".notmuch", f"notmuch-sync-{uuids["theirs"]}")

        changes = {}
        logger.info("Computing local changes...")
        changes["mine"] = get_changes(dbw, revision, prefix, fname)

        async def send():
            logger.info("Sending local changes...")
            write(json.dumps(changes["mine"]).encode("utf-8"), to_stream)

        async def recv():
            logger.info("Receiving remote changes...")
            changes["theirs"] = json.loads(read(from_stream).decode("utf-8"))

        async def sync():
            await asyncio.gather(send(), recv())

        asyncio.run(sync())
        logger.info("Changes synced.")
        tchanges = sync_tags(dbw, changes["mine"], changes["theirs"])
        logger.info("Tags synced.")

        revision = dbw.revision()

    return (prefix, changes["mine"], changes["theirs"], tchanges, fname, revision)


def get_missing_files(changes_mine, changes_theirs, prefix, move_on_change=False):
    """
    Determine which files are missing locally compared to the remote, and handle
    file moves/copies based on SHA256 checksums. Delete any files that aren't
    there on the remote anymore. This never deletes a message, only duplicate
    files for a message.

    Args:
        changes_mine (dict): Local changes.
        changes_theirs (dict): Remote changes.
        prefix (str): Prefix path for filenames (notmuch config database.path).
        move_on_change: Whether to move file that has local and remote changes.
        This flag is used to prevent infinite loops where local has one file
        name and remote another file name (e.g. when running mbsync independently).

    Returns:
        tuple: (dict of missing files, number of local moves/copies, number of
                local deletions)
    """
    ret = {}
    mcchanges = 0
    dchanges = 0
    with notmuch2.Database() as db:
        for mid in changes_theirs:
            try:
                msg = db.find(mid)
                if msg.ghost:
                    ret[mid] = changes_theirs[mid]
                    continue
                fnames_theirs = [ f["name"] for f in changes_theirs[mid]["files"] ]
                fnames_mine = [ str(f).removeprefix(prefix) for f in msg.filenames() ]
                missing_mine = [ f for f in fnames_theirs if f not in fnames_mine ]
                if len(missing_mine) > 0:
                    hashes_mine = [{"name": str(f).removeprefix(prefix),
                                    "sha": digest(Path(f).read_bytes())}
                                    for f in msg.filenames()]
                    for f in changes_theirs[mid]["files"]:
                        if f["name"] in missing_mine:
                            # check if it has been moved/copied
                            matches = [ x for x in hashes_mine if f["sha"] == x["sha"] ]
                            if len(matches) > 0:
                                src = os.path.join(prefix, matches[0]["name"])
                                dst = os.path.join(prefix, f["name"])
                                if matches[0] in changes_theirs[mid]["files"]:
                                    mcchanges += 1
                                    logger.info("Copying %s to %s.", src, dst)
                                    Path(dst).parent.mkdir(parents=True, exist_ok=True)
                                    shutil.copy(src, dst)
                                    fnames_mine.append(f["name"])
                                    with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
                                        dbw.add(dst)
                                elif mid not in changes_mine or move_on_change:
                                    mcchanges += 1
                                    logger.info("Moving %s to %s.", src, dst)
                                    Path(dst).parent.mkdir(parents=True, exist_ok=True)
                                    shutil.move(src, dst)
                                    fnames_mine.append(f["name"])
                                    fnames_mine.remove(matches[0]["name"])
                                    with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
                                        dbw.add(dst)
                                        logger.info("Removing %s from DB.", src)
                                        dbw.remove(src)
                                missing_mine.remove(f["name"])
                # check which ones are still missing
                if len(missing_mine) > 0:
                    ret[mid] = {"files": [f for f in changes_theirs[mid]["files"] if f["name"] in missing_mine]}

                # delete any files that are not there remotely after copy/move
                if mid not in changes_mine:
                    if len(set(fnames_mine).intersection(fnames_theirs)) == 0:
                        raise ValueError(f"Message '{mid}' has {fnames_theirs} on remote and different {fnames_mine} locally!")
                    to_delete = set(fnames_mine) - set(fnames_theirs)
                    with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
                        for f in to_delete:
                            fname = os.path.join(prefix, f)
                            dchanges += 1
                            logger.info("Removing %s from DB and deleting file.", fname)
                            dbw.remove(fname)
                            Path(fname).unlink()
            except LookupError:
                # don't have this message; all files missing
                ret[mid] = changes_theirs[mid]
    return (ret, mcchanges, dchanges)


def send_file(fname, stream):
    """
    Send a file's contents to a stream with 4-byte length prefix.

    Args:
        fname (str): Path to the file to send.
        stream: Writable stream.
    """
    with open(fname, "rb") as f:
        write(f.read(), stream)


def recv_file(fname, stream, sha=None, overwrite_raise=True):
    """
    Receive a file with a 4-byte length prefix from a stream and write it to
    disk, validating its checksum.

    Args:
        fname (str): Destination file path.
        stream: Readable stream.
        sha (str): Expected SHA256 checksum.
        overwrite_raise: Raise error if existing file would be overwritten.

    Raises:
        ValueError: If file to receive already exists or received file's
        checksum does not match expected.
    """
    content = read(stream)
    sha_mine = digest(content)
    if Path(fname).exists() and overwrite_raise:
        sha_exists = digest(Path(fname).read_bytes())
        if sha_exists != sha_mine:
            raise ValueError(f"Set to receive '{fname}', but already exists with different content!")
    if sha and sha_mine != sha:
        raise ValueError(f"Checksum of received file '{fname}' ({sha_mine}) does not match expected ({sha})!")
    Path(fname).parent.mkdir(parents=True, exist_ok=True)
    with open(fname, "wb") as f:
        f.write(content)


def sync_files(prefix, missing, from_stream, to_stream):
    """
    Synchronize files that are missing locally or remotely.

    Args:
        prefix (str): Prefix path for filenames (notmuch config database.path).
        missing (dict): Mapping of missing files by message ID.
        from_stream: Stream to read file names and files from.
        to_stream: Stream to send file names and files to.

    Returns:
        tuple: (number of locally added messages, number of locally added files)
    """
    files = {}
    files["mine"] = [ f | {"id": mid} for mid in missing for f in missing[mid]["files"] ]
    changes = {"files": len(files["mine"]), "messages": 0}

    async def send_fnames():
        logger.info("Sending file names missing on local...")
        to_stream.write(struct.pack("!I", len(files["mine"])))
        transfer["write"] += 4
        to_stream.flush()
        for f in files["mine"]:
            write(f["name"].encode("utf-8"), to_stream)

    async def recv_fnames():
        logger.info("Receiving file names missing on remote...")
        size_data = from_stream.read(4)
        transfer["read"] += 4
        nfiles = struct.unpack("!I", size_data)[0]
        files["theirs"] = [ read(from_stream).decode("utf-8") for _ in range(nfiles) ]

    async def sync_fnames():
        await asyncio.gather(send_fnames(), recv_fnames())

    asyncio.run(sync_fnames())
    logger.info("Missing file names synced.")

    async def send_files():
        for idx, fname in enumerate(files["theirs"]):
            logger.info("%s/%s Sending %s...", idx + 1, len(files["theirs"]),
                        fname)
            send_file(os.path.join(prefix, fname), to_stream)

    async def recv_files():
        for idx, f in enumerate(files["mine"]):
            logger.info("%s/%s Receiving %s...", idx + 1, len(files["mine"]), f["name"])
            dst = os.path.join(prefix, f["name"])
            recv_file(dst, from_stream, f["sha"])
            with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
                logger.info("Adding %s to DB.", dst)
                msg, dup = dbw.add(dst)
                if not dup:
                    changes["messages"] += 1
                    with msg.frozen():
                        logger.info("Setting tags %s for received %s.",
                                    sorted(missing[f["id"]]["tags"]),
                                    msg.messageid)
                        msg.tags.clear()
                        for tag in missing[f["id"]]["tags"]:
                            msg.tags.add(tag)

    async def sync():
        await asyncio.gather(send_files(), recv_files())

    asyncio.run(sync())
    logger.info("Missing files synced.")

    return (changes["messages"], changes["files"])


# Separate methods for local and remote to avoid sending all IDs both ways --
# have local figure out what needs to be deleted on both sides
def sync_deletes_local(from_stream, to_stream, no_check=False):
    """
    Synchronize deletions for the local database and instruct remote to delete
    messages/files as needed.

    Args:
        from_stream: Stream to read from the remote.
        to_stream: Stream to write to the remote.
        no_check: Delete message not present on other side even if it doesn't
        have the 'deleted' tag.

    Returns:
        int: Number of deletions performed locally.
    """
    ids = {}
    dels = {'a': 0}

    async def get_ids():
        with notmuch2.Database() as db:
            logger.info("Getting all message IDs from DB...")
            msgs = db.messages('*')
            ids["mine"] = [ msg.messageid for msg in msgs ]

    async def recv_ids():
        logger.info("Receiving all message IDs from remote...")
        size_data = from_stream.read(4)
        transfer["read"] += 4
        nids = struct.unpack("!I", size_data)[0]
        ids["theirs"] = [ read(from_stream).decode("utf-8") for _ in range(nids) ]

    async def sync_ids():
        await asyncio.gather(get_ids(), recv_ids())

    asyncio.run(sync_ids())
    logger.info("Message IDs synced.")

    async def send_del_ids():
        to_del_remote = set(ids["theirs"]) - set(ids["mine"])
        logger.debug("Remote IDs to be deleted %s.", to_del_remote)
        logger.info("Sending message IDs to be deleted to remote...")
        to_stream.write(struct.pack("!I", len(to_del_remote)))
        transfer["write"] += 4
        to_stream.flush()
        for mid in to_del_remote:
            write(mid.encode("utf-8"), to_stream)

    async def del_ids():
        to_del = set(ids["mine"]) - set(ids["theirs"])
        logger.debug("Local IDs to be deleted %s.", to_del)
        with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
            for mid in to_del:
                try:
                    msg = dbw.find(mid)
                    if msg.ghost:
                        continue
                    if "deleted" in msg.tags or no_check:
                        dels["a"] += 1
                        logger.info("Removing %s from DB and deleting files.", mid)
                        for f in msg.filenames():
                            logger.debug("Removing %s.", f)
                            dbw.remove(f)
                            Path(f).unlink()
                    else:
                        # not there on remote, but no "deleted" tag -- assume
                        # that something went wrong and set tags again to make
                        # it show up in next changeset to be synced back to
                        # remote
                        logger.info("%s set to be removed, but not tagged 'deleted'!", mid)
                        with msg.frozen():
                            tmp = "".join(msg.tags)
                            msg.tags.add(tmp)
                            msg.tags.discard(tmp)
                except LookupError:
                    # already deleted? doesn't matter
                    pass

    async def sync_dels():
        await asyncio.gather(send_del_ids(), del_ids())

    asyncio.run(sync_dels())
    return dels["a"]


def sync_deletes_remote(from_stream, to_stream, no_check=False):
    """
    Receive instructions from local to delete messages/files from the remote database.

    Args:
        from_stream: Stream to read from the local.
        to_stream: Stream to write to the local.
        no_check: Delete message not present on other side even if it doesn't
        have the 'deleted' tag.

    Returns:
        int: Number of deletions performed remotely.
    """
    dels = 0
    with notmuch2.Database() as db:
        msgs = db.messages('*')
        ids = [ msg.messageid for msg in msgs ]

    to_stream.write(struct.pack("!I", len(ids)))
    transfer["write"] += 4
    to_stream.flush()
    for i in ids:
        write(i.encode("utf-8"), to_stream)

    size_data = from_stream.read(4)
    transfer["read"] += 4
    nids = struct.unpack("!I", size_data)[0]
    to_del = [ read(from_stream).decode("utf-8") for _ in range(nids) ]

    with notmuch2.Database(mode=notmuch2.Database.MODE.READ_WRITE) as dbw:
        for mid in to_del:
            try:
                msg = dbw.find(mid)
                if msg.ghost:
                    continue
                if "deleted" in msg.tags or no_check:
                    dels += 1
                    for f in msg.filenames():
                        dbw.remove(f)
                        Path(f).unlink()
                else:
                    # not on local, but no "deleted" tag -- assume that
                    # something went wrong and set tags again to make it
                    # show up in next changeset to be synced back to local
                    with msg.frozen():
                        tmp = "".join(msg.tags)
                        msg.tags.add(tmp)
                        msg.tags.discard(tmp)
            except LookupError:
                # already deleted? doesn't matter
                pass
    return dels


def sync_mbsync_local(prefix, from_stream, to_stream):
    """
    Synchronize local mbsync files with remote.

    Args:
        prefix (str): Prefix path for filenames (notmuch config database.path).
        from_stream: Stream to read from the remote.
        to_stream: Stream to write to the remote.
    """
    mbsync = {}

    async def get_mbsync():
        logger.info("Getting local mbsync file stats...")
        mbsync["mine"] = { str(f).removeprefix(prefix): f.stat().st_mtime
                           for pat in [".uidvalidity", ".mbsyncstate"]
                           for f in Path(prefix).rglob(pat) }

    async def recv_mbsync():
        logger.info("Receiving mbsync file stats from remote...")
        mbsync["theirs"] = json.loads(read(from_stream).decode("utf-8"))

    async def sync_mbsync():
        await asyncio.gather(get_mbsync(), recv_mbsync())

    asyncio.run(sync_mbsync())
    logger.info("mbsync file stats synced.")

    pull = [ f for f in mbsync["mine"].keys()
            if (f in mbsync["theirs"] and mbsync["theirs"][f] > mbsync["mine"][f]) ]
    pull += list(set(mbsync["theirs"].keys()) - set(mbsync["mine"].keys()))
    logger.debug("Local mbsync files to be updated from remote %s.", pull)
    logger.info("Sending list of requested mbsync files to remote...")
    write(json.dumps(pull).encode("utf-8"), to_stream)

    async def send_mbsync_files():
        push = [ f for f in mbsync["theirs"].keys()
                if (f in mbsync["mine"] and mbsync["mine"][f] > mbsync["theirs"][f]) ]
        push += list(set(mbsync["mine"].keys()) - set(mbsync["theirs"].keys()))

        logger.debug("mbsync files to update on remote %s.", push)
        logger.info("Sending mbsync files to remote...")
        write(json.dumps(push).encode("utf-8"), to_stream)
        for idx, f in enumerate(push):
            logger.debug("%s/%s Sending mbsync file %s to remote...", idx + 1,
                         len(push), f)
            send_file(os.path.join(prefix, f), to_stream)

    async def recv_mbsync_files():
        for idx, f in enumerate(pull):
            logger.debug("%s/%s Receiving mbsync file %s from remote...",
                         idx + 1, len(pull), f)
            recv_file(os.path.join(prefix, f), from_stream,
                      overwrite_raise=False)

    async def sync_mbsync_files():
        await asyncio.gather(send_mbsync_files(), recv_mbsync_files())

    asyncio.run(sync_mbsync_files())
    logger.info("mbsync files synced.")


def sync_mbsync_remote(prefix, from_stream, to_stream):
    """
    Synchronize remote mbsync files with local.

    Args:
        prefix (str): Prefix path for filenames (notmuch config database.path).
        from_stream: Stream to read from the remote.
        to_stream: Stream to write to the remote.
    """
    mbsync = { str(f).removeprefix(prefix): f.stat().st_mtime
               for pat in [".uidvalidity", ".mbsyncstate"]
               for f in Path(prefix).rglob(pat) }
    write(json.dumps(mbsync).encode("utf-8"), to_stream)

    push = json.loads(read(from_stream).decode("utf-8"))

    async def send_mbsync_files():
        for f in push:
            send_file(os.path.join(prefix, f), to_stream)

    async def recv_mbsync_files():
        pull = json.loads(read(from_stream).decode("utf-8"))
        for f in pull:
            recv_file(os.path.join(prefix, f), from_stream,
                      overwrite_raise=False)

    async def sync_mbsync_files():
        await asyncio.gather(send_mbsync_files(), recv_mbsync_files())

    asyncio.run(sync_mbsync_files())


def sync_remote(args):
    """
    Run synchronization in remote mode.

    Args:
        args: Parsed command-line arguments.
    """
    prefix, changes_mine, changes_theirs, tchanges, sync_fname, rev = initial_sync(sys.stdin.buffer, sys.stdout.buffer)
    missing, fchanges, dfchanges = get_missing_files(changes_mine, changes_theirs, prefix, move_on_change=False)
    rmessages, rfiles = sync_files(prefix, missing, sys.stdin.buffer, sys.stdout.buffer)
    # record previous sync version after transferring files -- if something goes
    # wrong while transferring files, we'd miss any new files during the next
    # sync otherwise
    # this will result in some changes (e.g. added files) being picked up in the
    # sync again, but we check whether actual changes have been made
    record_sync(sync_fname, rev)

    dchanges = 0
    if args.delete:
        dchanges = sync_deletes_remote(sys.stdin.buffer, sys.stdout.buffer, args.delete_no_check)
    if args.mbsync:
        sync_mbsync_remote(prefix, sys.stdin.buffer, sys.stdout.buffer)
    sys.stdout.buffer.write(struct.pack("!IIIIII", tchanges, fchanges, dfchanges,
                                        rmessages, dchanges, rfiles))
    sys.stdout.buffer.flush()


def sync_local(args):
    """
    Run synchronization in local mode, communicating with the remote over SSH or
    a custom command.

    Args:
        args: Parsed command-line arguments.
    """
    if args.remote_cmd:
        cmd = shlex.split(args.remote_cmd)
    else:
        rargs = [(f"{args.user}@" if args.user else "") + args.remote, f"{args.path}"]
        if args.delete:
            rargs.append("--delete")
        if args.delete_no_check:
            rargs.append("--delete-no-check")
        if args.mbsync:
            rargs.append("--mbsync")
        cmd = shlex.split(args.ssh_cmd) + rargs

    logger.info("Connecting to remote...")
    with subprocess.Popen(
                cmd,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            ) as proc:
        from_remote = proc.stdout
        err_remote = proc.stderr
        to_remote = proc.stdin

        data = b''
        try:
            prefix, changes_mine, changes_theirs, tchanges, sync_fname, rev = initial_sync(from_remote, to_remote)
            logger.debug("Local changes %s, remote changes %s.", changes_mine, changes_theirs)
            missing, fchanges, dfchanges  = get_missing_files(changes_mine, changes_theirs, prefix, move_on_change=True)
            logger.debug("Local missing files %s.", missing)
            rmessages, rfiles = sync_files(prefix, missing, from_remote, to_remote)
            # record previous sync version after transferring files -- if something goes
            # wrong while transferring files, we'd miss any new files during the next
            # sync otherwise
            # this will result in some changes (e.g. added files) being picked up in the
            # sync again, but we check whether actual changes have been made
            record_sync(sync_fname, rev)

            dchanges = 0
            if args.delete:
                dchanges = sync_deletes_local(from_remote, to_remote, args.delete_no_check)
            if args.mbsync:
                sync_mbsync_local(prefix, from_remote, to_remote)

            logger.info("Getting change numbers from remote...")
            remote_changes = struct.unpack("!IIIIII", from_remote.read(6 * 4))
            transfer["read"] += 6 * 4
        finally:
            ready, _, exc = select([err_remote], [], [], 0)
            if ready and not exc:
                data = err_remote.read()
                # getting zero data on EOF
                if len(data) > 0:
                    print(f"Remote error: {data}", file=sys.stderr)

        to_remote.close()
        from_remote.close()
        err_remote.close()

    logger.warning("local:\t%s new messages,\t%s new files,\t%s files copied/moved,\t%s files deleted,\t%s messages with tag changes,\t%s messages deleted", rmessages, rfiles, fchanges, dfchanges, tchanges, dchanges)
    logger.warning("remote:\t%s new messages,\t%s new files,\t%s files copied/moved,\t%s files deleted,\t%s messages with tag changes,\t%s messages deleted", remote_changes[3], remote_changes[5], remote_changes[1], remote_changes[2], remote_changes[0], remote_changes[4])
    logger.warning("%s/%s bytes received from/sent to remote.", transfer["read"], transfer["write"])

    if len(data) > 0:
        # error output from remote
        sys.exit(1)


def main():
    """
    Entry point for the command-line interface. Parses arguments and dispatches
    to local or remote sync.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("-r", "--remote", type=str, help="remote host to connect to")
    parser.add_argument("-u", "--user", type=str, help="SSH user to use")
    parser.add_argument("-v", "--verbose", action="count", default=0, help="increases verbosity, up to twice (ignored on remote)")
    parser.add_argument("-q", "--quiet", action="store_true", help="do not print any output, overrides --verbose")
    parser.add_argument("-s", "--ssh-cmd", type=str, default="ssh -CTaxq", help="SSH command to use")
    parser.add_argument("-m", "--mbsync", action="store_true", help="sync mbsync files (.mbsyncstate, .uidvalidity)")
    parser.add_argument("-p", "--path", type=str, default=os.path.basename(sys.argv[0]), help="path to notmuch-sync on remote server")
    parser.add_argument("-c", "--remote-cmd", type=str, help="command to run to sync; overrides --remote, --user, --ssh-cmd, --path")
    parser.add_argument("-d", "--delete", action="store_true", help="sync deleted messages (requires listing all messages in notmuch database, potentially expensive)")
    parser.add_argument("-x", "--delete-no-check", action="store_true", help="delete missing messages even if they don't have the 'deleted' tag -- potentially unsafe")
    args = parser.parse_args()

    if args.remote or args.remote_cmd:
        if args.verbose == 1:
            logger.setLevel(level=logging.INFO)
        elif args.verbose == 2:
            logger.setLevel(level=logging.DEBUG)
        else:
            logger.setLevel(level=logging.WARNING)

        if args.quiet:
            logger.disabled = True
        sync_local(args)
    else:
        logger.disabled = True
        sync_remote(args)


if __name__ == "__main__":
    main()
